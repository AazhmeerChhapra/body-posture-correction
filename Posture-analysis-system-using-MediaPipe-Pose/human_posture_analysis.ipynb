{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abd4498e",
   "metadata": {},
   "source": [
    "## Human Posture Detection\n",
    "\n",
    "[<img src =\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\" align=\"left\">](https://colab.research.google.com/github/spmallick/learnopencv/blob/master/Posture-analysis-system-using-MediaPipe-Pose/human_posture_analysis.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4b1872",
   "metadata": {},
   "source": [
    "Poor posture is a modern-day epidemic. Research has shown that children as young as 10 years of age are demonstrating spinal degeneration on x-ray. Certain postures, like forward head posture, have been linked to migraines, high blood pressure, and decreased lung capacity. In this notebook, you will learn to create an application to detect posture using mediapipe. We are going to use side view samples to perform analysis and draw conclusion. Practical application would require an webcam pointed at the side view of the person.\n",
    "\n",
    "### Workflow\n",
    " - Find point of interests (landmarks) to check angle.\n",
    " - Perform analysis on standard sample images.\n",
    " - Find threshold range for good and bad posture.\n",
    " - Apply on video/webcam input.\n",
    " \n",
    "### Goal\n",
    "To detect neck inclination and torso inclination as shown below.\n",
    "<br>\n",
    "<img src = \"https://learnopencv.com/wp-content/uploads/2022/03/mp-pose-03-posture-sitting-scaled.jpg\" align='center'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "546c5937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Downloading opencv_python-4.8.1.78-cp37-abi3-win_amd64.whl (38.1 MB)\n",
      "     ---------------------------------------- 38.1/38.1 MB 2.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.17.0 in d:\\anaconda\\envs\\pytorch2\\lib\\site-packages (from opencv-python) (1.23.5)\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.8.1.78\n",
      "Requirement already satisfied: mediapipe in d:\\anaconda\\envs\\pytorch2\\lib\\site-packages (0.10.0)\n",
      "Requirement already satisfied: numpy in d:\\anaconda\\envs\\pytorch2\\lib\\site-packages (from mediapipe) (1.23.5)\n",
      "Requirement already satisfied: attrs>=19.1.0 in d:\\anaconda\\envs\\pytorch2\\lib\\site-packages (from mediapipe) (22.1.0)\n",
      "Requirement already satisfied: absl-py in d:\\anaconda\\envs\\pytorch2\\lib\\site-packages (from mediapipe) (1.3.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in d:\\anaconda\\envs\\pytorch2\\lib\\site-packages (from mediapipe) (20210226132247)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in c:\\users\\aazhmeer chhapra\\appdata\\roaming\\python\\python39\\site-packages (from mediapipe) (0.4.6)\n",
      "Requirement already satisfied: matplotlib in d:\\anaconda\\envs\\pytorch2\\lib\\site-packages (from mediapipe) (3.8.0)\n",
      "Requirement already satisfied: opencv-contrib-python in c:\\users\\aazhmeer chhapra\\appdata\\roaming\\python\\python39\\site-packages (from mediapipe) (4.8.1.78)\n",
      "Requirement already satisfied: protobuf<4,>=3.11 in d:\\anaconda\\envs\\pytorch2\\lib\\site-packages (from mediapipe) (3.17.2)\n",
      "Requirement already satisfied: six>=1.9 in d:\\anaconda\\envs\\pytorch2\\lib\\site-packages (from protobuf<4,>=3.11->mediapipe) (1.16.0)\n",
      "Requirement already satisfied: CFFI>=1.0 in d:\\anaconda\\envs\\pytorch2\\lib\\site-packages (from sounddevice>=0.4.4->mediapipe) (1.15.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\anaconda\\envs\\pytorch2\\lib\\site-packages (from matplotlib->mediapipe) (4.39.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in d:\\anaconda\\envs\\pytorch2\\lib\\site-packages (from matplotlib->mediapipe) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in d:\\anaconda\\envs\\pytorch2\\lib\\site-packages (from matplotlib->mediapipe) (3.0.9)\n",
      "Requirement already satisfied: pillow>=6.2.0 in d:\\anaconda\\envs\\pytorch2\\lib\\site-packages (from matplotlib->mediapipe) (9.4.0)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\anaconda\\envs\\pytorch2\\lib\\site-packages (from matplotlib->mediapipe) (23.0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in d:\\anaconda\\envs\\pytorch2\\lib\\site-packages (from matplotlib->mediapipe) (5.12.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in d:\\anaconda\\envs\\pytorch2\\lib\\site-packages (from matplotlib->mediapipe) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\anaconda\\envs\\pytorch2\\lib\\site-packages (from matplotlib->mediapipe) (0.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\anaconda\\envs\\pytorch2\\lib\\site-packages (from matplotlib->mediapipe) (2.8.2)\n",
      "Requirement already satisfied: pycparser in d:\\anaconda\\envs\\pytorch2\\lib\\site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.21)\n",
      "Requirement already satisfied: zipp>=3.1.0 in d:\\anaconda\\envs\\pytorch2\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib->mediapipe) (3.15.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install opencv-python\n",
    "!pip install mediapipe\n",
    "!wget https://raw.githubusercontent.com/spmallick/learnopencv/blob/master/Posture-analysis-system-using-MediaPipe-Pose/input.mp4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e5ebdc",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45c4ca55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "import math as m\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "882e9fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initilize medipipe selfie segmentation class.\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_holistic = mp.solutions.holistic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0773cf81",
   "metadata": {},
   "source": [
    "## Function to calculate offset distance\n",
    "The setup requires the person to be in proper side view. **`findDistance`** function helps us determine offset distance between two poinst. It can be the hip points, the eyes or shoulder. As these points are always more or less symmetric about the central axis. With this, we are going to incorporate camera alignment assistance in the script. Distnace is calculated using the distance formula.\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "distance =  \\sqrt{(x2 - x1)^2+(y2 - y1)^2}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f510f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Dec 10 14:14:25 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 537.58                 Driver Version: 537.58       CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                     TCC/WDDM  | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce GTX 1660 Ti   WDDM  | 00000000:01:00.0  On |                  N/A |\n",
      "| N/A   47C    P8               8W /  80W |   1608MiB /  6144MiB |      3%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      1168    C+G   ...crosoft\\Edge\\Application\\msedge.exe    N/A      |\n",
      "|    0   N/A  N/A      1896    C+G   ...on\\119.0.2151.97\\msedgewebview2.exe    N/A      |\n",
      "|    0   N/A  N/A      1940    C+G   ....0_x64__8wekyb3d8bbwe\\HxOutlook.exe    N/A      |\n",
      "|    0   N/A  N/A      5516    C+G   ...AppData\\Roaming\\Spotify\\Spotify.exe    N/A      |\n",
      "|    0   N/A  N/A      8500    C+G   ...bs\\Common\\Client\\v1.0.7\\rsAppUI.exe    N/A      |\n",
      "|    0   N/A  N/A     11088    C+G   ...pIntegrations\\Grammarly.Desktop.exe    N/A      |\n",
      "|    0   N/A  N/A     11380    C+G   ...ience\\NVIDIA GeForce Experience.exe    N/A      |\n",
      "|    0   N/A  N/A     12124    C+G   ...71.0_x64__8wekyb3d8bbwe\\GameBar.exe    N/A      |\n",
      "|    0   N/A  N/A     12496    C+G   ...__8wekyb3d8bbwe\\Notepad\\Notepad.exe    N/A      |\n",
      "|    0   N/A  N/A     12656    C+G   ...siveControlPanel\\SystemSettings.exe    N/A      |\n",
      "|    0   N/A  N/A     13128    C+G   ...GeForce Experience\\NVIDIA Share.exe    N/A      |\n",
      "|    0   N/A  N/A     13952    C+G   ...5n1h2txyewy\\ShellExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     15052    C+G   ....CBS_cw5n1h2txyewy\\FESearchHost.exe    N/A      |\n",
      "|    0   N/A  N/A     16892    C+G   ...a\\AppData\\Roaming\\Zoom\\bin\\Zoom.exe    N/A      |\n",
      "|    0   N/A  N/A     16908    C+G   ...2txyewy\\StartMenuExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     17000    C+G   ...nt.CBS_cw5n1h2txyewy\\SearchHost.exe    N/A      |\n",
      "|    0   N/A  N/A     17220    C+G   ...bs\\Common\\Client\\v1.1.0\\rsAppUI.exe    N/A      |\n",
      "|    0   N/A  N/A     17960    C+G   ...ekyb3d8bbwe\\PhoneExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     18068    C+G   ...oogle\\Chrome\\Application\\chrome.exe    N/A      |\n",
      "|    0   N/A  N/A     18992    C+G   C:\\Windows\\explorer.exe                   N/A      |\n",
      "|    0   N/A  N/A     20848    C+G   ...crosoft Office\\Office16\\OUTLOOK.EXE    N/A      |\n",
      "|    0   N/A  N/A     20960    C+G   ...Programs\\Microsoft VS Code\\Code.exe    N/A      |\n",
      "|    0   N/A  N/A     21496    C+G   ...GeForce Experience\\NVIDIA Share.exe    N/A      |\n",
      "|    0   N/A  N/A     21648    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe    N/A      |\n",
      "|    0   N/A  N/A     22300    C+G   ...1930_x64__8wekyb3d8bbwe\\msteams.exe    N/A      |\n",
      "|    0   N/A  N/A     24404    C+G   ...0_x64__8wekyb3d8bbwe\\HxAccounts.exe    N/A      |\n",
      "|    0   N/A  N/A     27728    C+G   ...on\\119.0.2151.97\\msedgewebview2.exe    N/A      |\n",
      "|    0   N/A  N/A     28084    C+G   ...crosoft Office\\Office16\\WINWORD.EXE    N/A      |\n",
      "|    0   N/A  N/A     30560    C+G   ...on\\119.0.2151.97\\msedgewebview2.exe    N/A      |\n",
      "|    0   N/A  N/A     30648    C+G   ...on\\119.0.2151.97\\msedgewebview2.exe    N/A      |\n",
      "|    0   N/A  N/A     31224    C+G   ...137.0_x64__dt26b99r8h8gj\\RtkUWP.exe    N/A      |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceb4ca55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def findDistance(x1, y1, x2, y2):\n",
    "    dist = m.sqrt((x2-x1)**2+(y2-y1)**2)\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9634fd09",
   "metadata": {},
   "source": [
    "## Function to calculate angle subtended by the line of interest to y-axis\n",
    "This is the primary deterministic factor for the posture. Using the angle subtended by the neck line and the torso line to y-axis. The neck line connects the shoulder and the eye, here we take shoulder as the pivotal point. Similarly the torso line connects hip and the shoulder, where hip is considered pivotal point. \n",
    "<br>\n",
    "\n",
    "<img src=\"https://learnopencv.com/wp-content/uploads/2022/03/mp-pose-05-neckline-inclination.jpg\" alt=\"MediaPipe pose neck inclination\" align=\"middle\" width=\"500\" height=\"600\">\n",
    "\n",
    "<br>\n",
    "\n",
    "Taking neck line as an example, here points are $P_1(x_1, y_1)$(shoulder), $P_2(x_2, y_2)$ (eye) and $P_3(x_3, y_3)$ (any point on vertical axis passing through $P_1$). \n",
    "<br>\n",
    "Hence, for $P_3$ x-coordinate is same as to that of $P_1$ and since $y_3$ is valid for all y, let's take $y_3 = 0$ for simplicity. <br>To find the inner angle of three points, we take vector approach. Angle between two vectors $\\vec{P_{12}}$ and $\\vec{P_{13}}$ is given by,\n",
    "\\begin{align}\n",
    "\\theta = \\arccos (\\frac{\\vec{P_{12}}.\\vec{P_{13}}}{|\\vec{P_{12}}|.|\\vec{P_{13}}|})\n",
    "\\end{align}\n",
    "Solving for $\\theta$ we get, \n",
    "\\begin{align}\n",
    "\\theta = \\arccos (\\frac{y_1^2 - y_1.y_2}{y_1\\sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2}})\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4492e989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate angle.\n",
    "def findAngle(x1, y1, x2, y2):\n",
    "    theta = m.acos((y2 -y1)*(-y1) / (m.sqrt((x2 - x1)**2 + (y2 - y1)**2) * y1))\n",
    "    degree = int(180/m.pi)*theta\n",
    "    return degree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70dd6b4",
   "metadata": {},
   "source": [
    "## Function to send alert\n",
    "Use this function to send alerts when bad posture is detected. Feel free to get creative and customize as per your convenience. You can use telegram notifiction alert system, [chek out Telegram bot automation here](https://core.telegram.org/bots). Or you can take it up a notch by creating an android app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8db85c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sendWarning(x):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f81cc7",
   "metadata": {},
   "source": [
    "## Constants and Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb810117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initilize frame counters.\n",
    "good_frames = 0\n",
    "bad_frames = 0\n",
    "\n",
    "# Font type.\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "# Colors.\n",
    "blue = (255, 127, 0)\n",
    "red = (50, 50, 255)\n",
    "green = (127, 255, 0)\n",
    "dark_blue = (127, 20, 0)\n",
    "light_green = (127, 233, 100)\n",
    "yellow = (0, 255, 255)\n",
    "pink = (255, 0, 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb0eb7cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing..\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Initialize mediapipe pose class.\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose()\n",
    "\n",
    "# Initialize video capture object.\n",
    "# For webcam input replace file name with 0.\n",
    "# file_name = 'input.mp4'\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Meta.\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "frame_size = (width, height)\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "\n",
    "# Initialize video writer.\n",
    "video_output = cv2.VideoWriter('output.mp4', fourcc, fps, frame_size)\n",
    "print('Processing..')\n",
    "while True:\n",
    "    # Capture frames.\n",
    "    success, image = cap.read()\n",
    "\n",
    "    if not success:\n",
    "        print(\"Null.Frames\")\n",
    "        break\n",
    "    # Get fps.\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    # Get height and width.\n",
    "    h, w = image.shape[:2]\n",
    "\n",
    "    # Convert the BGR image to RGB.\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Process the image.\n",
    "    keypoints = pose.process(image)\n",
    "\n",
    "    # Convert the image back to BGR.\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # Use lm and lmPose as representative of the following methods.\n",
    "    lm = keypoints.pose_landmarks\n",
    "    lmPose = mp_pose.PoseLandmark\n",
    "\n",
    "    # Acquire the landmark coordinates.\n",
    "    # Once aligned properly, left or right should not be a concern.      \n",
    "    # Left shoulder.\n",
    "    l_shldr_x = int(lm.landmark[lmPose.LEFT_SHOULDER].x * w)\n",
    "    l_shldr_y = int(lm.landmark[lmPose.LEFT_SHOULDER].y * h)\n",
    "    # Right shoulder\n",
    "    r_shldr_x = int(lm.landmark[lmPose.RIGHT_SHOULDER].x * w)\n",
    "    r_shldr_y = int(lm.landmark[lmPose.RIGHT_SHOULDER].y * h)\n",
    "    # Left ear.\n",
    "    l_ear_x = int(lm.landmark[lmPose.LEFT_EAR].x * w)\n",
    "    l_ear_y = int(lm.landmark[lmPose.LEFT_EAR].y * h)\n",
    "    # Left hip.\n",
    "    l_hip_x = int(lm.landmark[lmPose.LEFT_HIP].x * w)\n",
    "    l_hip_y = int(lm.landmark[lmPose.LEFT_HIP].y * h)\n",
    "\n",
    "    # Calculate distance between left shoulder and right shoulder points.\n",
    "    offset = findDistance(l_shldr_x, l_shldr_y, r_shldr_x, r_shldr_y)\n",
    "\n",
    "    # Assist to align the camera to point at the side view of the person.\n",
    "    # Offset threshold 30 is based on results obtained from analysis over 100 samples.\n",
    "    if offset < 100:\n",
    "        cv2.putText(image, str(int(offset)) + ' Aligned', (w - 150, 30), font, 0.9, green, 2)\n",
    "    else:\n",
    "        cv2.putText(image, str(int(offset)) + ' Not Aligned', (w - 150, 30), font, 0.9, red, 2)\n",
    "\n",
    "    # Calculate angles.\n",
    "    neck_inclination = findAngle(l_shldr_x, l_shldr_y, l_ear_x, l_ear_y)\n",
    "    torso_inclination = findAngle(l_hip_x, l_hip_y, l_shldr_x, l_shldr_y)\n",
    "\n",
    "    # Draw landmarks.\n",
    "    cv2.circle(image, (l_shldr_x, l_shldr_y), 7, yellow, -1)\n",
    "    cv2.circle(image, (l_ear_x, l_ear_y), 7, yellow, -1)\n",
    "\n",
    "    # Let's take y - coordinate of P3 100px above x1,  for display elegance.\n",
    "    # Although we are taking y = 0 while calculating angle between P1,P2,P3.\n",
    "    cv2.circle(image, (l_shldr_x, l_shldr_y - 100), 7, yellow, -1)\n",
    "    cv2.circle(image, (r_shldr_x, r_shldr_y), 7, pink, -1)\n",
    "    cv2.circle(image, (l_hip_x, l_hip_y), 7, yellow, -1)\n",
    "\n",
    "    # Similarly, here we are taking y - coordinate 100px above x1. Note that\n",
    "    # you can take any value for y, not necessarily 100 or 200 pixels.\n",
    "    cv2.circle(image, (l_hip_x, l_hip_y - 100), 7, yellow, -1)\n",
    "\n",
    "    # Put text, Posture and angle inclination.\n",
    "    # Text string for display.\n",
    "    angle_text_string = 'Neck : ' + str(int(neck_inclination)) + '  Torso : ' + str(int(torso_inclination))\n",
    "\n",
    "    # Determine whether good posture or bad posture.\n",
    "    # The threshold angles have been set based on intuition.\n",
    "    if neck_inclination < 40 and torso_inclination < 10:\n",
    "        bad_frames = 0\n",
    "        good_frames += 1\n",
    "\n",
    "        cv2.putText(image, angle_text_string, (10, 30), font, 0.9, light_green, 2)\n",
    "        cv2.putText(image, str(int(neck_inclination)), (l_shldr_x + 10, l_shldr_y), font, 0.9, light_green, 2)\n",
    "        cv2.putText(image, str(int(torso_inclination)), (l_hip_x + 10, l_hip_y), font, 0.9, light_green, 2)\n",
    "\n",
    "        # Join landmarks.\n",
    "        cv2.line(image, (l_shldr_x, l_shldr_y), (l_ear_x, l_ear_y), green, 4)\n",
    "        cv2.line(image, (l_shldr_x, l_shldr_y), (l_shldr_x, l_shldr_y - 100), green, 4)\n",
    "        cv2.line(image, (l_hip_x, l_hip_y), (l_shldr_x, l_shldr_y), green, 4)\n",
    "        cv2.line(image, (l_hip_x, l_hip_y), (l_hip_x, l_hip_y - 100), green, 4)\n",
    "\n",
    "    else:\n",
    "        good_frames = 0\n",
    "        bad_frames += 1\n",
    "\n",
    "        cv2.putText(image, angle_text_string, (10, 30), font, 0.9, red, 2)\n",
    "        cv2.putText(image, str(int(neck_inclination)), (l_shldr_x + 10, l_shldr_y), font, 0.9, red, 2)\n",
    "        cv2.putText(image, str(int(torso_inclination)), (l_hip_x + 10, l_hip_y), font, 0.9, red, 2)\n",
    "\n",
    "        # Join landmarks.\n",
    "        cv2.line(image, (l_shldr_x, l_shldr_y), (l_ear_x, l_ear_y), red, 4)\n",
    "        cv2.line(image, (l_shldr_x, l_shldr_y), (l_shldr_x, l_shldr_y - 100), red, 4)\n",
    "        cv2.line(image, (l_hip_x, l_hip_y), (l_shldr_x, l_shldr_y), red, 4)\n",
    "        cv2.line(image, (l_hip_x, l_hip_y), (l_hip_x, l_hip_y - 100), red, 4)\n",
    "\n",
    "    # Calculate the time of remaining in a particular posture.\n",
    "    good_time = (1 / fps) * good_frames\n",
    "    bad_time =  (1 / fps) * bad_frames\n",
    "\n",
    "    # Pose time.\n",
    "    if good_time > 0:\n",
    "        time_string_good = 'Good Posture Time : ' + str(round(good_time, 1)) + 's'\n",
    "        cv2.putText(image, time_string_good, (10, h - 20), font, 0.9, green, 2)\n",
    "    else:\n",
    "        time_string_bad = 'Bad Posture Time : ' + str(round(bad_time, 1)) + 's'\n",
    "        cv2.putText(image, time_string_bad, (10, h - 20), font, 0.9, red, 2)\n",
    "\n",
    "    # If you stay in bad posture for more than 3 minutes (180s) send an alert.\n",
    "    if bad_time > 180:\n",
    "        sendWarning()\n",
    "    # Write frames.\n",
    "    video_output.write(image)\n",
    "print('Finished.')\n",
    "cap.release()\n",
    "video_output.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing..\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "# Initialize mediapipe pose class.\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose()\n",
    "\n",
    "# Initialize video capture object.\n",
    "# For webcam input replace file name with 0.\n",
    "# file_name = 'input.mp4'\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Meta.\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "frame_size = (width, height)\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "\n",
    "# Initialize video writer.\n",
    "video_output = cv2.VideoWriter('output.mp4', fourcc, fps, frame_size)\n",
    "print('Processing..')\n",
    "while True:\n",
    "    # Capture frames.\n",
    "    success, image = cap.read()\n",
    "\n",
    "    if not success:\n",
    "        print(\"Null.Frames\")\n",
    "        break\n",
    "\n",
    "    # Get fps.\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    # Get height and width.\n",
    "    h, w = image.shape[:2]\n",
    "\n",
    "    # Convert the BGR image to RGB.\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Process the image.\n",
    "    keypoints = pose.process(image)\n",
    "\n",
    "    # Convert the image back to BGR.\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    # Use lm and lmPose as representative of the following methods.\n",
    "    lm = keypoints.pose_landmarks\n",
    "    lmPose = mp_pose.PoseLandmark\n",
    "\n",
    "    # Acquire the landmark coordinates.\n",
    "    # Once aligned properly, left or right should not be a concern.      \n",
    "    # Left shoulder.\n",
    "    l_shldr_x = int(lm.landmark[lmPose.LEFT_SHOULDER].x * w)\n",
    "    l_shldr_y = int(lm.landmark[lmPose.LEFT_SHOULDER].y * h)\n",
    "    # Right shoulder\n",
    "    r_shldr_x = int(lm.landmark[lmPose.RIGHT_SHOULDER].x * w)\n",
    "    r_shldr_y = int(lm.landmark[lmPose.RIGHT_SHOULDER].y * h)\n",
    "    # Left ear.\n",
    "    l_ear_x = int(lm.landmark[lmPose.LEFT_EAR].x * w)\n",
    "    l_ear_y = int(lm.landmark[lmPose.LEFT_EAR].y * h)\n",
    "    # Left hip.\n",
    "    l_hip_x = int(lm.landmark[lmPose.LEFT_HIP].x * w)\n",
    "    l_hip_y = int(lm.landmark[lmPose.LEFT_HIP].y * h)\n",
    "\n",
    "    # Calculate distance between left shoulder and right shoulder points.\n",
    "    offset = findDistance(l_shldr_x, l_shldr_y, r_shldr_x, r_shldr_y)\n",
    "\n",
    "    # Assist to align the camera to point at the side view of the person.\n",
    "    # Offset threshold 30 is based on results obtained from analysis over 100 samples.\n",
    "    if offset < 100:\n",
    "        cv2.putText(image, str(int(offset)) + ' Aligned', (w - 150, 30), font, 0.9, green, 2)\n",
    "    else:\n",
    "        cv2.putText(image, str(int(offset)) + ' Not Aligned', (w - 150, 30), font, 0.9, red, 2)\n",
    "\n",
    "    # Calculate angles.\n",
    "    neck_inclination = findAngle(l_shldr_x, l_shldr_y, l_ear_x, l_ear_y)\n",
    "    torso_inclination = findAngle(l_hip_x, l_hip_y, l_shldr_x, l_shldr_y)\n",
    "\n",
    "    # Draw landmarks.\n",
    "    cv2.circle(image, (l_shldr_x, l_shldr_y), 7, yellow, -1)\n",
    "    cv2.circle(image, (l_ear_x, l_ear_y), 7, yellow, -1)\n",
    "\n",
    "    # Let's take y - coordinate of P3 100px above x1,  for display elegance.\n",
    "    # Although we are taking y = 0 while calculating angle between P1,P2,P3.\n",
    "    cv2.circle(image, (l_shldr_x, l_shldr_y - 100), 7, yellow, -1)\n",
    "    cv2.circle(image, (r_shldr_x, r_shldr_y), 7, pink, -1)\n",
    "    cv2.circle(image, (l_hip_x, l_hip_y), 7, yellow, -1)\n",
    "\n",
    "    # Similarly, here we are taking y - coordinate 100px above x1. Note that\n",
    "    # you can take any value for y, not necessarily 100 or 200 pixels.\n",
    "    cv2.circle(image, (l_hip_x, l_hip_y - 100), 7, yellow, -1)\n",
    "\n",
    "    # Put text, Posture and angle inclination.\n",
    "    # Text string for display.\n",
    "    angle_text_string = 'Neck : ' + str(int(neck_inclination)) + '  Torso : ' + str(int(torso_inclination))\n",
    "\n",
    "    # Determine whether good posture or bad posture.\n",
    "    # The threshold angles have been set based on intuition.\n",
    "    if neck_inclination < 40 and torso_inclination < 10:\n",
    "        bad_frames = 0\n",
    "        good_frames += 1\n",
    "\n",
    "        cv2.putText(image, angle_text_string, (10, 30), font, 0.9, light_green, 2)\n",
    "        cv2.putText(image, str(int(neck_inclination)), (l_shldr_x + 10, l_shldr_y), font, 0.9, light_green, 2)\n",
    "        cv2.putText(image, str(int(torso_inclination)), (l_hip_x + 10, l_hip_y), font, 0.9, light_green, 2)\n",
    "\n",
    "        # Join landmarks.\n",
    "        cv2.line(image, (l_shldr_x, l_shldr_y), (l_ear_x, l_ear_y), green, 4)\n",
    "        cv2.line(image, (l_shldr_x, l_shldr_y), (l_shldr_x, l_shldr_y - 100), green, 4)\n",
    "        cv2.line(image, (l_hip_x, l_hip_y), (l_shldr_x, l_shldr_y), green, 4)\n",
    "        cv2.line(image, (l_hip_x, l_hip_y), (l_hip_x, l_hip_y - 100), green, 4)\n",
    "\n",
    "    else:\n",
    "        good_frames = 0\n",
    "        bad_frames += 1\n",
    "\n",
    "        cv2.putText(image, angle_text_string, (10, 30), font, 0.9, red, 2)\n",
    "        cv2.putText(image, str(int(neck_inclination)), (l_shldr_x + 10, l_shldr_y), font, 0.9, red, 2)\n",
    "        cv2.putText(image, str(int(torso_inclination)), (l_hip_x + 10, l_hip_y), font, 0.9, red, 2)\n",
    "\n",
    "        # Join landmarks.\n",
    "        cv2.line(image, (l_shldr_x, l_shldr_y), (l_ear_x, l_ear_y), red, 4)\n",
    "        cv2.line(image, (l_shldr_x, l_shldr_y), (l_shldr_x, l_shldr_y - 100), red, 4)\n",
    "        cv2.line(image, (l_hip_x, l_hip_y), (l_shldr_x, l_shldr_y), red, 4)\n",
    "        cv2.line(image, (l_hip_x, l_hip_y), (l_hip_x, l_hip_y - 100), red, 4)\n",
    "\n",
    "    # Calculate the time of remaining in a particular posture.\n",
    "    good_time = (1 / fps) * good_frames\n",
    "    bad_time =  (1 / fps) * bad_frames\n",
    "\n",
    "    # Pose time.\n",
    "    if good_time > 0:\n",
    "        time_string_good = 'Good Posture Time : ' + str(round(good_time, 1)) + 's'\n",
    "        cv2.putText(image, time_string_good, (10, h - 20), font, 0.9, green, 2)\n",
    "    else:\n",
    "        time_string_bad = 'Bad Posture Time : ' + str(round(bad_time, 1)) + 's'\n",
    "        cv2.putText(image, time_string_bad, (10, h - 20), font, 0.9, red, 2)\n",
    "\n",
    "    # If you stay in bad posture for more than 3 minutes (180s) send an alert.\n",
    "    if bad_time > 180:\n",
    "        sendWarning()\n",
    "        # Show the resulting image with landmarks.\n",
    "    cv2.imshow(\"Posture Analysis\", image)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "    # Write frames.\n",
    "    # video_output.write(image)\n",
    "cap.release()\n",
    "video_output.release()\n",
    "cv2.destroyAllWindows()\n",
    "print('Finished.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bb920c58",
   "metadata": {},
   "source": [
    "## Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00af6213",
   "metadata": {},
   "source": [
    "Processing the image using mediapipe is fairly simple, after setting minimum detection confidence and minimum tracking confidence, we need to pass the RGB image to `pose.process()` method. The documentation can be found in this [**link**](https://google.github.io/mediapipe/solutions/pose). With the acquired result, we find the coordinates of specific landmarks. Then we find the angles suntended by the line of interset to y-axis and draw conclusion on the basis of results obtained from analysis script. The code is self explanatory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dcfa56b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing..\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'landmark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mf:\\opencv\\Posture-analysis-system-using-MediaPipe-Pose\\human_posture_analysis.ipynb Cell 18\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/opencv/Posture-analysis-system-using-MediaPipe-Pose/human_posture_analysis.ipynb#X23sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m lmPose \u001b[39m=\u001b[39m mp_pose\u001b[39m.\u001b[39mPoseLandmark\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/opencv/Posture-analysis-system-using-MediaPipe-Pose/human_posture_analysis.ipynb#X23sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39m# Acquire the landmark coordinates.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/opencv/Posture-analysis-system-using-MediaPipe-Pose/human_posture_analysis.ipynb#X23sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39m# Once aligned properly, left or right should not be a concern.      \u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/opencv/Posture-analysis-system-using-MediaPipe-Pose/human_posture_analysis.ipynb#X23sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m# Left shoulder.\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/f%3A/opencv/Posture-analysis-system-using-MediaPipe-Pose/human_posture_analysis.ipynb#X23sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m l_shldr_x \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(lm\u001b[39m.\u001b[39;49mlandmark[lmPose\u001b[39m.\u001b[39mLEFT_SHOULDER]\u001b[39m.\u001b[39mx \u001b[39m*\u001b[39m w)\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/opencv/Posture-analysis-system-using-MediaPipe-Pose/human_posture_analysis.ipynb#X23sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m l_shldr_y \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(lm\u001b[39m.\u001b[39mlandmark[lmPose\u001b[39m.\u001b[39mLEFT_SHOULDER]\u001b[39m.\u001b[39my \u001b[39m*\u001b[39m h)\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/opencv/Posture-analysis-system-using-MediaPipe-Pose/human_posture_analysis.ipynb#X23sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39m# Right shoulder\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'landmark'"
     ]
    }
   ],
   "source": [
    "print('Processing..')\n",
    "while cap.isOpened():\n",
    "    # Capture frames.\n",
    "    success, image = cap.read()\n",
    "    if not success:\n",
    "        print(\"Null.Frames\")\n",
    "        break\n",
    "    # Get fps.\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    # Get height and width.\n",
    "    h, w = image.shape[:2]\n",
    "\n",
    "    # Convert the BGR image to RGB.\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Process the image.\n",
    "    keypoints = pose.process(image)\n",
    "\n",
    "    # Convert the image back to BGR.\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # Use lm and lmPose as representative of the following methods.\n",
    "    lm = keypoints.pose_landmarks\n",
    "    lmPose = mp_pose.PoseLandmark\n",
    "\n",
    "    # Acquire the landmark coordinates.\n",
    "    # Once aligned properly, left or right should not be a concern.      \n",
    "    # Left shoulder.\n",
    "    l_shldr_x = int(lm.landmark[lmPose.LEFT_SHOULDER].x * w)\n",
    "    l_shldr_y = int(lm.landmark[lmPose.LEFT_SHOULDER].y * h)\n",
    "    # Right shoulder\n",
    "    r_shldr_x = int(lm.landmark[lmPose.RIGHT_SHOULDER].x * w)\n",
    "    r_shldr_y = int(lm.landmark[lmPose.RIGHT_SHOULDER].y * h)\n",
    "    # Left ear.\n",
    "    l_ear_x = int(lm.landmark[lmPose.LEFT_EAR].x * w)\n",
    "    l_ear_y = int(lm.landmark[lmPose.LEFT_EAR].y * h)\n",
    "    # Left hip.\n",
    "    l_hip_x = int(lm.landmark[lmPose.LEFT_HIP].x * w)\n",
    "    l_hip_y = int(lm.landmark[lmPose.LEFT_HIP].y * h)\n",
    "\n",
    "    # Calculate distance between left shoulder and right shoulder points.\n",
    "    offset = findDistance(l_shldr_x, l_shldr_y, r_shldr_x, r_shldr_y)\n",
    "\n",
    "    # Assist to align the camera to point at the side view of the person.\n",
    "    # Offset threshold 30 is based on results obtained from analysis over 100 samples.\n",
    "    if offset < 100:\n",
    "        cv2.putText(image, str(int(offset)) + ' Aligned', (w - 150, 30), font, 0.9, green, 2)\n",
    "    else:\n",
    "        cv2.putText(image, str(int(offset)) + ' Not Aligned', (w - 150, 30), font, 0.9, red, 2)\n",
    "\n",
    "    # Calculate angles.\n",
    "    neck_inclination = findAngle(l_shldr_x, l_shldr_y, l_ear_x, l_ear_y)\n",
    "    torso_inclination = findAngle(l_hip_x, l_hip_y, l_shldr_x, l_shldr_y)\n",
    "\n",
    "    # Draw landmarks.\n",
    "    cv2.circle(image, (l_shldr_x, l_shldr_y), 7, yellow, -1)\n",
    "    cv2.circle(image, (l_ear_x, l_ear_y), 7, yellow, -1)\n",
    "\n",
    "    # Let's take y - coordinate of P3 100px above x1,  for display elegance.\n",
    "    # Although we are taking y = 0 while calculating angle between P1,P2,P3.\n",
    "    cv2.circle(image, (l_shldr_x, l_shldr_y - 100), 7, yellow, -1)\n",
    "    cv2.circle(image, (r_shldr_x, r_shldr_y), 7, pink, -1)\n",
    "    cv2.circle(image, (l_hip_x, l_hip_y), 7, yellow, -1)\n",
    "\n",
    "    # Similarly, here we are taking y - coordinate 100px above x1. Note that\n",
    "    # you can take any value for y, not necessarily 100 or 200 pixels.\n",
    "    cv2.circle(image, (l_hip_x, l_hip_y - 100), 7, yellow, -1)\n",
    "\n",
    "    # Put text, Posture and angle inclination.\n",
    "    # Text string for display.\n",
    "    angle_text_string = 'Neck : ' + str(int(neck_inclination)) + '  Torso : ' + str(int(torso_inclination))\n",
    "\n",
    "    # Determine whether good posture or bad posture.\n",
    "    # The threshold angles have been set based on intuition.\n",
    "    if neck_inclination < 40 and torso_inclination < 10:\n",
    "        bad_frames = 0\n",
    "        good_frames += 1\n",
    "\n",
    "        cv2.putText(image, angle_text_string, (10, 30), font, 0.9, light_green, 2)\n",
    "        cv2.putText(image, str(int(neck_inclination)), (l_shldr_x + 10, l_shldr_y), font, 0.9, light_green, 2)\n",
    "        cv2.putText(image, str(int(torso_inclination)), (l_hip_x + 10, l_hip_y), font, 0.9, light_green, 2)\n",
    "\n",
    "        # Join landmarks.\n",
    "        cv2.line(image, (l_shldr_x, l_shldr_y), (l_ear_x, l_ear_y), green, 4)\n",
    "        cv2.line(image, (l_shldr_x, l_shldr_y), (l_shldr_x, l_shldr_y - 100), green, 4)\n",
    "        cv2.line(image, (l_hip_x, l_hip_y), (l_shldr_x, l_shldr_y), green, 4)\n",
    "        cv2.line(image, (l_hip_x, l_hip_y), (l_hip_x, l_hip_y - 100), green, 4)\n",
    "\n",
    "    else:\n",
    "        good_frames = 0\n",
    "        bad_frames += 1\n",
    "\n",
    "        cv2.putText(image, angle_text_string, (10, 30), font, 0.9, red, 2)\n",
    "        cv2.putText(image, str(int(neck_inclination)), (l_shldr_x + 10, l_shldr_y), font, 0.9, red, 2)\n",
    "        cv2.putText(image, str(int(torso_inclination)), (l_hip_x + 10, l_hip_y), font, 0.9, red, 2)\n",
    "\n",
    "        # Join landmarks.\n",
    "        cv2.line(image, (l_shldr_x, l_shldr_y), (l_ear_x, l_ear_y), red, 4)\n",
    "        cv2.line(image, (l_shldr_x, l_shldr_y), (l_shldr_x, l_shldr_y - 100), red, 4)\n",
    "        cv2.line(image, (l_hip_x, l_hip_y), (l_shldr_x, l_shldr_y), red, 4)\n",
    "        cv2.line(image, (l_hip_x, l_hip_y), (l_hip_x, l_hip_y - 100), red, 4)\n",
    "\n",
    "    # Calculate the time of remaining in a particular posture.\n",
    "    good_time = (1 / fps) * good_frames\n",
    "    bad_time =  (1 / fps) * bad_frames\n",
    "\n",
    "    # Pose time.\n",
    "    if good_time > 0:\n",
    "        time_string_good = 'Good Posture Time : ' + str(round(good_time, 1)) + 's'\n",
    "        cv2.putText(image, time_string_good, (10, h - 20), font, 0.9, green, 2)\n",
    "    else:\n",
    "        time_string_bad = 'Bad Posture Time : ' + str(round(bad_time, 1)) + 's'\n",
    "        cv2.putText(image, time_string_bad, (10, h - 20), font, 0.9, red, 2)\n",
    "\n",
    "    # If you stay in bad posture for more than 3 minutes (180s) send an alert.\n",
    "    if bad_time > 180:\n",
    "        sendWarning()\n",
    "    # Write frames.\n",
    "    video_output.write(image)\n",
    "\n",
    "     # Display the resulting image.\n",
    "    cv2.imshow(\"Posture Analysis\", image)\n",
    "\n",
    "    # Break the loop when 'q' key is pressed.\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "print('Finished.')\n",
    "cap.release()\n",
    "video_output.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
